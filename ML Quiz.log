This is pdfTeX, Version 3.1415926-2.4-1.40.13 (MiKTeX 2.9) (preloaded format=pdflatex 2013.6.13)  30 NOV 2013 11:11
entering extended mode
**ML*Quiz.tex
("C:\Users\Computer5\Dropbox\Public\ML-Coursera\ML Quiz.tex"
LaTeX2e <2011/06/27>
Babel <v3.8m> and hyphenation patterns for english, afrikaans, ancientgreek, ar
abic, armenian, assamese, basque, bengali, bokmal, bulgarian, catalan, coptic, 
croatian, czech, danish, dutch, esperanto, estonian, farsi, finnish, french, ga
lician, german, german-x-2012-05-30, greek, gujarati, hindi, hungarian, iceland
ic, indonesian, interlingua, irish, italian, kannada, kurmanji, latin, latvian,
 lithuanian, malayalam, marathi, mongolian, mongolianlmc, monogreek, ngerman, n
german-x-2012-05-30, nynorsk, oriya, panjabi, pinyin, polish, portuguese, roman
ian, russian, sanskrit, serbian, slovak, slovenian, spanish, swedish, swissgerm
an, tamil, telugu, turkish, turkmen, ukenglish, ukrainian, uppersorbian, usengl
ishmax, welsh, loaded.
("C:\Program Files\MiKTeX 2.9\tex\latex\base\article.cls"
Document Class: article 2007/10/19 v1.4h Standard LaTeX document class
("C:\Program Files\MiKTeX 2.9\tex\latex\base\size10.clo"
File: size10.clo 2007/10/19 v1.4h Standard LaTeX file (size option)
)
\c@part=\count79
\c@section=\count80
\c@subsection=\count81
\c@subsubsection=\count82
\c@paragraph=\count83
\c@subparagraph=\count84
\c@figure=\count85
\c@table=\count86
\abovecaptionskip=\skip41
\belowcaptionskip=\skip42
\bibindent=\dimen102
)
("C:\Program Files\MiKTeX 2.9\tex\latex\amsmath\amsmath.sty"
Package: amsmath 2013/01/14 v2.14 AMS math features
\@mathmargin=\skip43

For additional information on amsmath, use the `?' option.
("C:\Program Files\MiKTeX 2.9\tex\latex\amsmath\amstext.sty"
Package: amstext 2000/06/29 v2.01

("C:\Program Files\MiKTeX 2.9\tex\latex\amsmath\amsgen.sty"
File: amsgen.sty 1999/11/30 v2.0
\@emptytoks=\toks14
\ex@=\dimen103
))
("C:\Program Files\MiKTeX 2.9\tex\latex\amsmath\amsbsy.sty"
Package: amsbsy 1999/11/29 v1.2d
\pmbraise@=\dimen104
)
("C:\Program Files\MiKTeX 2.9\tex\latex\amsmath\amsopn.sty"
Package: amsopn 1999/12/14 v2.01 operator names
)
\inf@bad=\count87
LaTeX Info: Redefining \frac on input line 210.
\uproot@=\count88
\leftroot@=\count89
LaTeX Info: Redefining \overline on input line 306.
\classnum@=\count90
\DOTSCASE@=\count91
LaTeX Info: Redefining \ldots on input line 378.
LaTeX Info: Redefining \dots on input line 381.
LaTeX Info: Redefining \cdots on input line 466.
\Mathstrutbox@=\box26
\strutbox@=\box27
\big@size=\dimen105
LaTeX Font Info:    Redeclaring font encoding OML on input line 566.
LaTeX Font Info:    Redeclaring font encoding OMS on input line 567.
\macc@depth=\count92
\c@MaxMatrixCols=\count93
\dotsspace@=\muskip10
\c@parentequation=\count94
\dspbrk@lvl=\count95
\tag@help=\toks15
\row@=\count96
\column@=\count97
\maxfields@=\count98
\andhelp@=\toks16
\eqnshift@=\dimen106
\alignsep@=\dimen107
\tagshift@=\dimen108
\tagwidth@=\dimen109
\totwidth@=\dimen110
\lineht@=\dimen111
\@envbody=\toks17
\multlinegap=\skip44
\multlinetaggap=\skip45
\mathdisplay@stack=\toks18
LaTeX Info: Redefining \[ on input line 2665.
LaTeX Info: Redefining \] on input line 2666.
)
("C:\Program Files\MiKTeX 2.9\tex\latex\amsfonts\amssymb.sty"
Package: amssymb 2013/01/14 v3.01 AMS font symbols

("C:\Program Files\MiKTeX 2.9\tex\latex\amsfonts\amsfonts.sty"
Package: amsfonts 2013/01/14 v3.01 Basic AMSFonts support
\symAMSa=\mathgroup4
\symAMSb=\mathgroup5
LaTeX Font Info:    Overwriting math alphabet `\mathfrak' in version `bold'
(Font)                  U/euf/m/n --> U/euf/b/n on input line 106.
))
(C:\Users\Computer5\AppData\Roaming\MiKTeX\2.9\tex\latex\framed\framed.sty
Package: framed 2011/10/22 v 0.96: framed or shaded text with page breaks
\OuterFrameSep=\skip46
\fb@frw=\dimen112
\fb@frh=\dimen113
\FrameRule=\dimen114
\FrameSep=\dimen115
)
("C:\Users\Computer5\Dropbox\Public\ML-Coursera\ML Quiz.aux")
LaTeX Font Info:    Checking defaults for OML/cmm/m/it on input line 7.
LaTeX Font Info:    ... okay on input line 7.
LaTeX Font Info:    Checking defaults for T1/cmr/m/n on input line 7.
LaTeX Font Info:    ... okay on input line 7.
LaTeX Font Info:    Checking defaults for OT1/cmr/m/n on input line 7.
LaTeX Font Info:    ... okay on input line 7.
LaTeX Font Info:    Checking defaults for OMS/cmsy/m/n on input line 7.
LaTeX Font Info:    ... okay on input line 7.
LaTeX Font Info:    Checking defaults for OMX/cmex/m/n on input line 7.
LaTeX Font Info:    ... okay on input line 7.
LaTeX Font Info:    Checking defaults for U/cmr/m/n on input line 7.
LaTeX Font Info:    ... okay on input line 7.
LaTeX Font Info:    Try loading font information for U+msa on input line 13.

("C:\Program Files\MiKTeX 2.9\tex\latex\amsfonts\umsa.fd"
File: umsa.fd 2013/01/14 v3.01 AMS symbols A
)
LaTeX Font Info:    Try loading font information for U+msb on input line 13.

("C:\Program Files\MiKTeX 2.9\tex\latex\amsfonts\umsb.fd"
File: umsb.fd 2013/01/14 v3.01 AMS symbols B
)
Overfull \hbox (263.99469pt too wide) in paragraph at lines 24--24
[]\OT1/cmtt/m/n/10 This is not true, because depending on the initial condition
, gradient descent may end up at different local optima.[] 
 []


Overfull \hbox (174.74547pt too wide) in paragraph at lines 24--24
[]\OT1/cmtt/m/n/10 YES If the learning rate is too small, then gradient descent
 may take a very long time to converge.[] 
 []


Overfull \hbox (479.24281pt too wide) in paragraph at lines 24--24
[]    \OT1/cmtt/m/n/10 If the learning rate is small, gradient descent ends up 
taking an extremely small step on each iteration, and therefore can take a long
 time to converge.[] 
 []


Overfull \hbox (279.74455pt too wide) in paragraph at lines 30--30
[]\OT1/cmtt/m/n/10 YES If $\theta_0$ and $\theta_1$ are initialized at the glob
al minimum, the one iteration will not change their values.[] 
 []


Overfull \hbox (248.24483pt too wide) in paragraph at lines 30--30
[]    \OT1/cmtt/m/n/10 At the global minimum, the derivative (gradient) is zero
, so gradient descent will not change the parameters.[] 
 []


Overfull \hbox (332.2441pt too wide) in paragraph at lines 36--36
[]\OT1/cmtt/m/n/10 NO Setting the learning rate $\alpha$ to be very small is no
t harmful, and can only speed up the convergence of gradient descent.[] 
 []


Overfull \hbox (694.49094pt too wide) in paragraph at lines 36--36
[]   \OT1/cmtt/m/n/10 If the learning rate is small, gradient descent ends up t
aking an extremely small step on each iteration, so this would actually slow do
wn (rather than speed up) the convergence of the algorithm.[] 
 []


Overfull \hbox (48.74657pt too wide) in paragraph at lines 43--43
[]\OT1/cmtt/m/n/10 If ?0 and ?1 are initialized at a local minimum, the one ite
ration will not[] 
 []


Overfull \hbox (43.49661pt too wide) in paragraph at lines 43--43
[]\OT1/cmtt/m/n/10 At a local minimum, the derivative (gradient) is zero, so gr
adient descent[] 
 []


Overfull \hbox (300.74437pt too wide) in paragraph at lines 49--49
[]\OT1/cmtt/m/n/10 If gradient descent instead increases the objective value, t
hat means alpha is too large (or you have a bug in your code!).[] 
 []


Overfull \hbox (195.74529pt too wide) in paragraph at lines 54--54
[]\OT1/cmtt/m/n/10 YES If ?0 and ?1 are initialized at the global minimum, the 
one iteration will not change their values.
 []


Overfull \hbox (295.49442pt too wide) in paragraph at lines 54--54
\OT1/cmtt/m/n/10 Correct 0.25 At the global minimum, the derivative (gradient) 
is zero, so gradient descent will not change the parameters.[] 
 []


Overfull \hbox (452.99304pt too wide) in paragraph at lines 54--54
[]\OT1/cmtt/m/n/10 YES No matter how ?0 and ?1 are initialized, so long as a is
 sufficiently small, we can safely expect gradient descent to converge to the s
ame solution.
 []


Overfull \hbox (332.2441pt too wide) in paragraph at lines 54--54
\OT1/cmtt/m/n/10 Correct 0.25 This is not true, because depending on the initia
l condition, gradient descent may end up at different local optima.[] 
 []


Overfull \hbox (274.4946pt too wide) in paragraph at lines 58--58
[]\OT1/cmtt/m/n/10 NO Even if the learning rate a is very large, every iteratio
n of gradient descent will decrease the value of f(?0,?1).
 []


Overfull \hbox (473.99286pt too wide) in paragraph at lines 58--58
\OT1/cmtt/m/n/10 Inorrect 0.00 If the learning rate a is too large, one step of
 gradient descent can actually vastly "overshoot", and actuall increase the val
ue of f(?0,?1).[] 
 []


Overfull \hbox (657.74126pt too wide) in paragraph at lines 62--62
[]\OT1/cmtt/m/n/10 YES If ?0 and ?1 are initialized so that ?0=?1, then by symm
etry (because we do simultaneous updates to the two parameters), after one iter
ation of gradient descent, we will still have ?0=?1.
 []


Overfull \hbox (720.7407pt too wide) in paragraph at lines 62--62
\OT1/cmtt/m/n/10 Inorrect 0.00 The updates to ?0 and ?1 are different (even tho
ugh we're doing simultaneous updates), so there's no particular reason to expec
t them to be the same after one iteration of gradient descent.[] 
 []


Overfull \hbox (463.49295pt too wide) in paragraph at lines 69--69
[]\OT1/cmtt/m/n/10 Suppose that for some linear regression problem (say, predic
ting housing prices as in the lecture), we have some training set, and for our 
training set we[] 
 []


Overfull \hbox (458.243pt too wide) in paragraph at lines 69--69
[]\OT1/cmtt/m/n/10 managed to find some $\theta_0$, $\theta_1$ such that $J(\th
eta_0, \theta_1)$=0. Which of the statements below must then be true? (Check al
l that apply.)[] 
 []


Overfull \hbox (584.2419pt too wide) in paragraph at lines 73--73
[]\OT1/cmtt/m/n/10 NO We can perfectly predict the value of y even for new exam
ples that we have not yet seen. (e.g., we can perfectly predict prices of even 
new houses that we have not yet seen.)
 []


Overfull \hbox (636.74144pt too wide) in paragraph at lines 73--73
\OT1/cmtt/m/n/10 Inorrect 0.00 Even though we can fit our training set perfectl
y, this does not mean that we'll always make perfect predictions on houses in t
he future/on houses that we have not yet seen.[] 
 []


Overfull \hbox (295.49442pt too wide) in paragraph at lines 76--76
[]\OT1/cmtt/m/n/10 NO This is not possible: By the definition of J(?0,?1), it i
s not possible for there to exist ?0 and ?1 so that J(?0,?1)=0
 []


Overfull \hbox (169.49551pt too wide) in paragraph at lines 76--76
\OT1/cmtt/m/n/10 Correct 0.25 If all of our training examples lie perfectly on 
a line, then J(?0,?1)=0 is possible.[] 
 []

[1

{C:/Users/Computer5/AppData/Local/MiKTeX/2.9/pdftex/config/pdftex.map}]
Overfull \hbox (353.24391pt too wide) in paragraph at lines 82--82
[]\OT1/cmtt/m/n/10 YES Our training set can be fit perfectly by a straight line
, i.e., all of our training examples lie perfectly on some straight line.
 []


Overfull \hbox (258.74474pt too wide) in paragraph at lines 82--82
\OT1/cmtt/m/n/10 Inorrect 0.00 If J(?0,?1)=0, that means the line defined by th
e equation "y=?0+?1x" perfectly fits all of our data.[] 
 []


Overfull \hbox (169.49551pt too wide) in paragraph at lines 82--82
[]\OT1/cmtt/m/n/10 NO Gradient descent is likely to get stuck at a local minimu
m and fail to find the global minimum.
 []


Overfull \hbox (589.49185pt too wide) in paragraph at lines 82--82
\OT1/cmtt/m/n/10 Inorrect 0.00 The cost function J(?0,?1) for linear regression
 has no local optima (other than the global minimum), so gradient descent will 
not get stuck at a bad local minimum.[] 
 []

Missing character: There is no â in font cmtt10!
Missing character: There is no € in font cmtt10!
Missing character: There is no ¦ in font cmtt10!

Overfull \hbox (32.9967pt too wide) in paragraph at lines 87--87
[]\OT1/cmtt/m/n/10 NO For this to be true, we must have y(i)=0 for every value 
of i=1,2,,m.
 []


Overfull \hbox (636.74144pt too wide) in paragraph at lines 87--87
\OT1/cmtt/m/n/10 Correct 0.25 So long as all of our training examples lie on a 
straight line, we will be able to find ?0 and ?1 so that J(?0,?1)=0. It is not 
necessary that y(i)=0 for all of our examples.[] 
 []


Overfull \hbox (127.49588pt too wide) in paragraph at lines 87--87
[]\OT1/cmtt/m/n/10 NO For this to be true, we must have $\theta_0$=0 and $\thet
a_1$=0 so that $h_\theta$(x)=0
 []


Overfull \hbox (174.74547pt too wide) in paragraph at lines 103--103
[]\OT1/cmtt/m/n/10 YES Gradient descent is likely to get stuck at a local minim
um and fail to find the global minimum.[] 
 []


Overfull \hbox (358.49387pt too wide) in paragraph at lines 103--103
[]   \OT1/cmtt/m/n/10 Incorrect 0.00 The cost function $J(\theta_0, \theta_1)$ 
for linear regression has no local optima (other than the global minimum),[] 
 []


Overfull \hbox (347.99396pt too wide) in paragraph at lines 112--112
[]\OT1/cmtt/m/n/10 NO Our training set can be fit perfectly by a straight line,
 i.e., all of our training examples lie perfectly on some straight line.[] 
 []


Overfull \hbox (295.49442pt too wide) in paragraph at lines 112--112
[]   \OT1/cmtt/m/n/10 If $J(\theta_0, \theta_1)$=0, that means the line defined
 by the equation "y=$\theta_0$+$\theta_1$x" perfectly fits all[] 
 []


Overfull \hbox (305.99432pt too wide) in paragraph at lines 112--112
[]\OT1/cmtt/m/n/10 YES For these values of ?0 and ?1 that satisfy J(?0,?1)=0, w
e have that h?(x(i))=y(i) for every training example (x(i),y(i))
 []


Overfull \hbox (242.99487pt too wide) in paragraph at lines 112--112
\OT1/cmtt/m/n/10 Inorrect 0.00 J(?0,?1)=0, that means the line defined by the e
quation "y=?0+?1x" perfectly fits all of our data.[] 
 []

[2] ("C:\Users\Computer5\Dropbox\Public\ML-Coursera\ML Quiz.aux") ) 
Here is how much of TeX's memory you used:
 1152 strings out of 493921
 13136 string characters out of 3147270
 67654 words of memory out of 3000000
 4489 multiletter control sequences out of 15000+200000
 5507 words of font info for 23 fonts, out of 3000000 for 9000
 841 hyphenation exceptions out of 8191
 27i,4n,26p,410b,154s stack positions out of 5000i,500n,10000p,200000b,50000s
<C:/Program 
Files/MiKTeX 2.9/fonts/type1/public/amsfonts/cm/cmmi10.pfb><C:/Program Files/Mi
KTeX 2.9/fonts/type1/public/amsfonts/cm/cmr10.pfb><C:/Program Files/MiKTeX 2.9/
fonts/type1/public/amsfonts/cm/cmr7.pfb><C:/Program Files/MiKTeX 2.9/fonts/type
1/public/amsfonts/cm/cmtt10.pfb>
Output written on "ML Quiz.pdf" (2 pages, 59627 bytes).
PDF statistics:
 25 PDF objects out of 1000 (max. 8388607)
 0 named destinations out of 1000 (max. 500000)
 1 words of extra memory for PDF output out of 10000 (max. 10000000)

