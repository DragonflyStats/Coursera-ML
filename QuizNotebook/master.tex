You'd like to use polynomial regression to predict a student's ჌▘nal exam score
from their midterm exam score. Concretely, suppose you want to ჌▘t a model
of the form , where is the midterm score and
is (midterm score) . Further, you plan to use both feature scaling (dividing
by the "max-min", or range, of a feature) and mean normalization.
What is the normalized feature ? (Hint: midterm = 72, fial = 74 is training
example 2.) 


Please round your answer to two decimal places and enter in
the text box below.

%----------------------------------------%

Suppose you have training examples with features (excluding
the additional all-ones feature for the intercept term, which you should add).
The normal equation is . For the given values of and
, what are the dimensions of , , and in this equation?

%---------------------------------------%
\subsection{Question 4}
Suppose you have a dataset with m = 1000000 examples and n=200000
features for each example. You want to use multivariate linear regression to
fit the parameters $\theta$ to our data. Should you prefer gradient descent or the
normal equation?



1. 
Suppose m=4 students have taken some class, and the class had a midterm exam and a final exam. You have collected a dataset of their scores on the two exams, which is as follows:

midterm exam	(midterm exam)2	final exam
89	7921	96
72	5184	74
94	8836	87
69	4761	78
You'd like to use polynomial regression to predict a student's final exam score from their midterm exam score. Concretely, suppose you want to fit a model of the form hθ(x)=θ0+θ1x1+θ2x2, where x1 is the midterm score and x2 is (midterm score)2. Further, you plan to use both feature scaling (dividing by the "max-min", or range, of a feature) and mean normalization.

What is the normalized feature x(1)1? (Hint: midterm = 89, final = 96 is training example 1.) Please round off your answer to two decimal places and enter in the text box below.

Enter answer here

%-------------------------------------------------------------------------%
2. 
You run gradient descent for 15 iterations

with α=0.3 and compute

J(θ) after each iteration. You find that the

value of J(θ) \textbfdecreases} quickly then levels

off. Based on this, which of the following conclusions seems most plausible?



α=0.3 is an effective choice of learning rate.  [YES]

Rather than use the current value of α, it'd be more promising to try a smaller value of α (say α=0.1).

Rather than use the current value of α, it'd be more promising to try a larger value of α (say α=1.0).

%-------------------------------------------------------------------------%
3. 
Suppose you have m=23 training examples with n=5 features (excluding the additional all-ones feature for the intercept term, which you should add). The normal equation is θ=(XTX)−1XTy. For the given values of m and n, what are the dimensions of θ, X, and y in this equation?

\begin{itemize}
\item X is 23×6, y is 23×1, θ is 6×1 [YES]

\item X is 23×5, y is 23×1, θ is 5×5

\itemX is 23×5, y is 23×1, θ is 5×1

\item X is 23×6, y is 23×6, θ is 6×6
\end{itemize}
%-------------------------------------------------------------------------%
4. 
Suppose you have a dataset with m=1000000 examples and n=200000 features for each example. You want to use multivariate linear regression to fit the parameters θ to our data. Should you prefer gradient descent or the normal equation?


\begin{itemize}
\item The normal equation, since it provides an efficient way to directly find the solution. [NO]

\item Gradient descent, since it will always converge to the optimal θ.

\item Gradient descent, since (XTX)−1 will be very slow to compute in the normal equation.

\item The normal equation, since gradient descent might be unable to find the optimal θ.
\end{itemize}
%-------------------------------------------------------------------------%
\subsection{ Question 5.} 
Which of the following are reasons for using feature scaling?

\begin{itemize}
\item It speeds up solving for θ using the normal equation.
\item It prevents the matrix $X^TX$ (used in the normal equation) from being non-invertable (singular/degenerate).
\item It speeds up gradient descent by making it require fewer iterations to get to a good solution.
\item It is necessary to prevent gradient descent from getting stuck in local optima.
\end{itemize}

\newpage
\section{Week 3 - Logistic Regression}

Failed
0/5 points earned (0%)
You haven't passed yet. You need at least 80% to pass.
Review the material and try again!  You have 3 attempts every 8 hours.
Review Related Lesson


%-------------------------------------------------------------------------%
\subsection{ 1. }
Suppose that you have trained a logistic regression classifier, and it outputs on a new example x a prediction hθ(x) = 0.4. This means (check all that apply):

%-------------------------------------------------------------------------%
\subsection{Question 2. }
Suppose you have the following training set, and fit a logistic regression classifier hθ(x)=g(θ0+θ1x1+θ2x2).



Which of the following are true? Check all that apply.


%-------------------------------------------------------------------------%
\subsection{ 3. }
For logistic regression, the gradient is given by ∂∂θjJ(θ)=1m∑mi=1(hθ(x(i))−y(i))x(i)j. Which of these is a correct gradient descent update for logistic regression with a learning rate of α? Check all that apply.


%-------------------------------------------------------------------------%
\subsection{ 4. }
Which of the following statements are true? Check all that apply.

Incorrect
%-------------------------------------------------------------------------%
\subsection{ 5. }
Suppose you train a logistic classifier hθ(x)=g(θ0+θ1x1+θ2x2). 
Suppose θ0=−6,θ1=0,θ2=1. 

Which of the following figures represents the decision boundary found by your classifier?


\section{ Regularization }

%----------------------------------------------------------------------------------------------%
1. 
You are training a classification model with logistic

regression. Which of the following statements are true? Check

all that apply.

\begin{itemize}
\item 
Adding a new feature to the model always results in equal or better performance on examples not in the training set.
\item 
Adding many new features to the model makes it more likely to overfit the training set.
\item 
Introducing regularization to the model always results in equal or better performance on examples not in the training set.
\item 
Introducing regularization to the model always results in equal or better performance on the training set.
\end{itemize}
%----------------------------------------------------------------------------------------------%
2. 
Suppose you ran logistic regression twice, once with $\lambda$=0, and once with $\lambda$=1. One of the times, you got

parameters $\theta$=[26.2965.41], and the other time you got

$\theta$=[2.751.32]. However, you forgot which value of

$\lambda$ corresponds to which value of $\theta$. Which one do you

think corresponds to $\lambda$=1?

$\theta$=[26.2965.41]

$\theta$=[2.751.32]  YES
%----------------------------------------------------------------------------------------------%
3. 
Which of the following statements about regularization are

true? Check all that apply.


\begin{itemize}
\item Because logistic regression outputs values 0≤h$\theta$(x)≤1, it's range of output values can only be "shrunk" slightly by regularization anyway, so regularization is generally not helpful for it.

\item YES Using too large a value of $\lambda$ can cause your hypothesis to overfit the data; this can be avoided by reducing $\lambda$.
\item 
NO Consider a classification problem. Adding regularization may cause your classifier to incorrectly classify some training examples (which it had correctly classified when not using regularization, i.e. when $\lambda$=0).
\item 
Using a very large value of $\lambda$ cannot hurt the performance of your hypothesis; the only reason we do not set $\lambda$ to be too large is to avoid numerical problems.
\end{itemize}
%----------------------------------------------------------------------------------------------%
4. 
In which one of the following figures do you think the hypothesis has overfit the training set?

Figure:



Figure:



Figure:



Figure:


%----------------------------------------------------------------------------------------------%
5. 
In which one of the following figures do you think the hypothesis has underfit the training set?

Figure:



Figure:



Figure:



Figure:


3 questions unanswered
Submit Quiz




%==========================================================================%
\newpage
\section{Week 4 Neural Networks: Representation}


Topics: Neural Networks: Representation

Programming Exercise 3 : Multi-class classification and neural networks.

%=================================================================%

\section{ML Week 5}
\subsection*{Overfitting and Regularization}

\begin{itemize}
\item If one neural network overfits the training set, one reasonable step is to
increase the regularization parameter $\lambda$.
\item For computational efficiency, after we performed gradient checking to verify that our back-propagation 
code is correct, we usually disable gradient checkking before using back-propagation to train
the network
\end{itemize}
%--------------------------%

\subsection*{Exercise}
Let $ J(\theta) = 3\theta^2 + 2$

Let $\theta = 1$ and $\epsilon = 0.01$

Use the formula to numerically compute an approximation to the derivative of $\theta$
at $theta = 1$

\[
\frac{J(\theta + \epsilon) - J(\theta + \epsilon)}{2\epsilon} 
= \frac{(3(1.01)^2 + 2$)-(3(0.99)^2 + 2$)}{0.002} 
= 9.003

\]
%=================================================================%
\newpage
\section{Unsupervised Learning}
%=================================================================%
\subsection*{Question 1. }
For which of the following tasks might K-means clustering be a suitable algorithm? Select all that apply.


%=================================================================%
\subsection*{Question 2.} 
Suppose we have three cluster centroids $\mu_1$=$[1 2]^T$, $\mu_2$=$[−3 0]^T$ and $\mu_3$=$[4 2]^T$. 
Furthermore, we have a training example x(i)=$[3 1]^T$. After a cluster assignment step, what will $c^{(i)}$ be?


%=================================================================%
\subsection*{Question 3.}
K-means is an iterative algorithm, and two of the following steps are repeatedly carried out in its inner-loop. Which two?


%=================================================================%
\subsection*{Question 4. }
Suppose you have an unlabeled dataset $\{x(1),\ldots,x(m)\}$. You run K-means with 50 different random
initializations, and obtain 50 different clusterings of the data. 
What is the recommended way for choosing which one of these 50 clusterings to use?

%=================================================================%
\subsection*{Question 5. }
Which of the following statements are true? Select all that apply.


%=================================================================%
\newpage
\section{ML Week 8 : Machine Learning Clustering}

Suppose you have an unlabelled set of observations$\{ x^{(1)},x^{(2)},x^{(3)},x^{(4)}, \ldots ,x^{(n)}\}$.
You run $k-$means with 50 random initializations and obtain 50 different clusterings solution of the data.
What is the recommended way of choosing which solution to use.

\subsection{The Distortion Function}
For each of the clustering solutions, the distortion function is computed as 
\[ \frac{1}{m} \sum^{m}_{i=1} \parallel x^{(i)} - \mu_c^{(i)}\parallel ^2 \]
The optimal clustering solution is the solution that minizes the distortion function.
Since a lower value for a distortion function, implies a better clustering, you should choose the clustering with 
smallest value of the distortion function.
%----------------------------------------------------------------------------------%
\subsection*{Finding Closest Centroids}

In the cluste assignment phase of the algorithm, the algorithm assigns every training example $x^{(n)}$ to the closest 
centroid, given the current position of the centroids.

\[ C^{(i)} := J \mbox{ that minimizes } \| x^{} - \mu_j \| ^2 \]

\begin{itemize}
\item $C^{(i)}$ index of the centroid clostest to $x^{(i)}$
\item $\mu_j$ is the position of the $j-$th centroid.
\end{itemize}

%---------------%
\subsection{Computing Centroid Means}

\[ 
\mu_k := \frac{1}{abs(C_k)} \sum_{i \in C_k} x^{(i)}
\]
%---------------%

A good way to initialize k-mean is to select $k$ distinct examples from the training set and set the cluster centroids equal to these selected examples.

On every iteration of $k-$means, the cost function $J(C^{(1)},C^{(2)},\ldots, C^{(n)},
\mu_1,\ldots \mu_k)$

The distortion function should either stay the same or decrease, in particular it should not increase.

%---------------%
\subsection{Recommended Applications of PCA}

\begin{itemize}
\item Data Compression: reducing the dimensionof input data $x^{(i)}$, which will be used in a supervised learning algorithm
(i.e. use PCA data so that your supervised learning algorithm runs faster.

\item Data Visualization: Reduce data to 2D ( or 3D) so that it can be plotted.
\end{itemize}

\section{Principal Component Analysis}


\subsection{ Question 1. }

Consider the following 2D dataset:


Which of the following figures correspond to possible values that PCA may return for u(1) (the first eigenvector / first principal component)? 
Check all that apply (you may have to check more than one figure).

(Select both parallel vectors - CORRECT)







%====================================================================%

\subsection{ Question 2. }
Which of the following is a reasonable way to select the number of principal components k?

(Recall that n is the dimensionality of the input data and m is the number of input examples.)

\begin{itemize}
\item Choose the value of k that minimizes the approximation error 1m∑mi=1||x(i)−x(i)approx||2.

\item Choose k to be the smallest value so that at least 1\% of the variance is retained.

\item Choose k to be 99\% of n (i.e., k=0.99∗n, rounded to the nearest integer).

\item Choose k to be the smallest value so that at least 99% of the variance is retained. [CORRECT - Selected]

\end{itemize}
%====================================================================%
\subsection{ Question 3. }
Suppose someone tells you that they ran PCA in such a way that "95\% of the variance was retained." What is an equivalent statement to this?

1m∑mi=1||x(i)−x(i)approx||21m∑mi=1||x(i)||2≥0.95 [INCORRECT Selected]

1m∑mi=1||x(i)−x(i)approx||21m∑mi=1||x(i)||2≤0.05

1m∑mi=1||x(i)||21m∑mi=1||x(i)−x(i)approx||2≥0.95

1m∑mi=1||x(i)−x(i)approx||21m∑mi=1||x(i)||2≥0.05

%====================================================================%
\subsection{ Question 4. }
Which of the following statements are true? Check all that apply.

\begin{itemize}
\item Given an input x $in$ $\mathbb{R}^n$, PCA compresses it to a lower-dimensional vector z$in$ $\mathbb{R}^k$.

\item PCA can be used only to reduce the dimensionality of data by 1 (such as 3D to 2D, or 2D to 1D).

\item If the input features are on very different scales, it is a good idea to perform feature scaling before applying PCA.

\item Feature scaling is not useful for PCA, since the eigenvector calculation (such as using Octave's svd(Sigma) routine) takes care of this automatically.
\end{itemize}
%====================================================================%
\subsection{ Question 5. }
Which of the following are recommended applications of PCA? Select all that apply.

\begin{itemize}
\item [CORRECT] Data compression: Reduce the dimension of your data, so that it takes up less memory / disk space.

\item [CORRECT] Data compression: Reduce the dimension of your input data x(i), which will be used in a supervised learning algorithm (i.e., use PCA so that your supervised learning algorithm runs faster).

\item As a replacement for (or alternative to) linear regression: For most learning applications, PCA and linear regression give substantially similar results.

\item Data visualization: To take 2D data, and find a different way of plotting it in 2D (using k=2).

\end{itemize}
%====================================================================%

\section{Anomaly Detection}

%--------------------------------------------------------------------%
\subsection*{Question 1. }
For which of the following problems would \textbf{anomaly detection} be a suitable algorithm?

\begin{itemize}
\item From a large set of hospital patient records, predict which patients have a particular disease (say, the flu).
\item From a large set of primary care patient records, identify individuals who might have unusual health conditions.
\item CORRECT
Given data from credit card transactions, classify each transaction according to type of purchase (for example: food, transportation, clothing).
\item 
In a computer chip fabrication plant, identify microchips that might be defective.
\end{itemize}

%--------------------------------------------------------------------%
\subsection*{Question 2. Variant A}
Suppose you have trained an anomaly detection system that flags anomalies when p(x) is less than $\epsilon$, and you find on the cross-validation set that it has 
too many false negatives (failing to flag a lot of anomalies). What should you do?


\begin{itemize}
\item Increase $\epsilon$ [CORRECT]
\item Decrease $\epsilon$
\end{itemize}

\subsection*{Question 2. Variant B}
Suppose you have trained an anomaly detection system for fraud detection, and your system that flags anomalies when p(x) is less than ε, and you find on the 
cross-validation set that it mis-flagging far too many good transactions as fradulent. What should you do?

\begin{itemize}
\item Increase $\epsilon$ 
\item Decrease $\epsilon$ [CORRECT]
\end{itemize}

%--------------------------------------------------------------------%
\subsection*{Question 3. }
Suppose you are developing an anomaly detection system to catch manufacturing defects in airplane engines. You model uses

\[p(x)=∏nj=1p(xj;\muj,\sigma^2_j).\]

You have two features $x_1$ = vibration intensity, and $x_2$ = heat generated. 

Both x1 and x2 take on values between 0 and 1 (and are strictly greater than 0), and for most "normal" engines you expect that $x_1 \approx x_2$. 

One of the suspected anomalies is that a flawed engine may vibrate very intensely even without generating much heat (large x1, small x2), even though the particular values of x1 and x2 may not fall outside their typical ranges of values. What additional feature x3 should you create to capture these types of anomalies:

\begin{itemize}
\item x3=x1/x2 [CORRECT]

\item x3=x21×x2

\item x3=x1×x2

\item  x3=x1+x2

\end{itemize}

%--------------------------------------------------------------------%
\subsection*{Question 4. }
Which of the following are true? Check all that apply.

\begin{itemize}
\item When evaluating an anomaly detection algorithm on the cross validation set (containing some positive and some negative examples), classification accuracy is usually a good evaluation metric to use.
\item  In anomaly detection, we fit a model p(x) to a set of negative (y=0) examples, without using any positive examples we may have collected of previously observed anomalies.
\item \textbf{CORRECT} When developing an anomaly detection system, it is often useful to select an appropriate numerical performance metric to evaluate the effectiveness of the learning algorithm.
\item In a typical anomaly detection setting, we have a large number of anomalous examples, and a relatively small number of normal/non-anomalous examples.
\end{itemize}
%--------------------------------------------------------------------%
\subsection*{Question 5. }
You have a 1-D dataset $\{x(1),\ldots,x(m)\}$ and you want to detect outliers in the dataset. You first plot the dataset and it looks like this:


Suppose you fit the gaussian distribution parameters μ1 and σ21 to this dataset. Which of the following values for μ1 and σ21 might you get?
\begin{itemize}
\item μ1=−3,σ21=4
\item 
μ1=−6,σ21=4
\item
μ1=−3,σ21=2
\item 
μ1=−6,σ21=2
\end{itemize}



%------------------------------------------------%
% ML Week 9

\section{Recommender Systems}

Information Filtering System that attempts to recommend information items likely
to be of interest to a user.

\textbf{Commonly used algorithms}
\begin{itemize
\item $k-$means clustering
\item Pearson's Rho
\item Collaborative Filtering
\end{itemize}

Collaborative Filtering is the process of filtering for information or patterns using collaboration among multiple
agents.

Applications: online news aggregation or similar items of clothings

best approached by other methods - prediction

Collaborative Filtering Gradient
%NOT FINISHED

\[ \frac{\partial J}{\partial X^{(i)}_k}  = \sum [  ] \theta^{(j)}_k \]
\[ \frac{\partial J}{\partial \theta^{(i)}_k}  = \sum [  ] X^{(j)}_k \]

No regularization applied

%--------------------------------------%
Anomaly Detection
Gaussian Distribution
Estimate Gaussian Distribution

For $n$ feastures of $X$ , compute the mean and variance for each feature

%-----------%
Selecting Threshold of $\epsilon$.

Implement an algorithm to select the threshold $\epsilon$ using an $F_i$ score on a 
cross validation set.

$P(X) < \epsilon$ is considered to be an anomaly.

\subsection{$F_1$ Score}



\section*{ML Week 10 Large Scale Machine Learning}

\subsection*{Stochastic Gradient Checking}

\begin{itemize}
\item In each iteration of the stochastic gradient, the algorithm needs to examine/use only one training example.
\item Each iteration updates the parameters based on the cost of only one example $ Cost (\theta,(x^{(i)},y^{(i)}))$.
\item You can use numerical gradient checking to verify if your stochastic gradient descent implementation is bug-free.
\end{itemize}
